{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processing with ASteCA\n",
        "\n",
        "This notebook allows you to process the data for each cluster listed in the UCC using the [ASteCA](https://asteca.github.io/) package. The documentation of AsteCA has more details on how this tool can be used.\n",
        "\n",
        "The first step is to install the required ASteCA and [pyABC](https://pyabc.readthedocs.io/en/latest/) packages and import them along with other required packages."
      ],
      "metadata": {
        "id": "y6kRVlNPA1jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install latest published versions of ASteCA and PyABC\n",
        "!pip install asteca\n",
        "!pip install pyabc\n",
        "\n",
        "# Import required packages\n",
        "import pandas as pd\n",
        "from urllib.error import HTTPError\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tempfile\n",
        "import datetime as dt\n",
        "import asteca\n",
        "import pyabc"
      ],
      "metadata": {
        "id": "HRzvcKwABI3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we **define the name of the cluster to be analyzed**. In this example we select the cluster `Blanco 1` (it doesn't matter if you use upper or lower case):"
      ],
      "metadata": {
        "id": "XTnMW_YKF_kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster = \"blanco 1\""
      ],
      "metadata": {
        "id": "G8bmCAvIF_kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to load the cluster data into a `pandas` dataframe. To do this we first need to define the function that handles the loading.\n"
      ],
      "metadata": {
        "id": "SL_zhMhzF_kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cluster(cluster, db_id):\n",
        "    \"\"\"Function that handles loading a cluster's datafile in any of the\n",
        "    UCC, HUNT23 or CANTAT20 databases, from the UCC repositories.\"\"\"\n",
        "    # Proper file name format\n",
        "    file_name = cluster.lower().replace('_', '').replace(' ', '').replace('-', '')\n",
        "    # Proper databse name format\n",
        "    _db_id = '_'+db_id if db_id != 'UCC' else ''\n",
        "    # Check all repositories where datafiles are stored\n",
        "    for qfolder in ('1N', '1P', '2N', '2P', '3N', '3P', '4N', '4P'):\n",
        "        try:\n",
        "            repo_root = f\"https://github.com/ucc23/Q{qfolder}/raw/main/datafiles/\"\n",
        "            path = repo_root + file_name + f\"{_db_id}.parquet\"\n",
        "            df = pd.read_parquet(path)\n",
        "            print(f\"Cluster '{cluster}' loaded from '{db_id}' database\")\n",
        "            return df\n",
        "        except HTTPError:\n",
        "            pass\n",
        "    raise ValueError(f\"Cluster '{cluster}' not found in {db_id} database\")"
      ],
      "metadata": {
        "id": "0zg4avPrF_kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now load a cluster with data from the UCC, [Hunt & Reffert (2023)](https://ui.adsabs.harvard.edu/abs/2023A%26A...673A.114H) (HUNT23) or [Cantat-Gaudin et al. (2020)](https://ui.adsabs.harvard.edu/abs/2020A%26A...640A...1C) (CANTAT20), assuming that cluster was processed in either of those articles. To do this select either `UCC`, `HUNT23` or `CANTAT20` in the `db_id` variable below and load."
      ],
      "metadata": {
        "id": "7bsDSZNvF_kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_id = 'UCC'\n",
        "# db_id = 'HUNT23'\n",
        "# db_id = 'CANTAT20'\n",
        "\n",
        "# Load the dataframe\n",
        "df = load_cluster(cluster, db_id)"
      ],
      "metadata": {
        "id": "k8t9xjSWF_kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most probable members in the UCC database are stored using a probability cut of `P>0.5`. A minimum number of member stars is set to `25`, so that if less than `25` stars have `P>0.5` then the most probable members are those `25` stars with the largest probability values below 0.5. Other databases handle this differently."
      ],
      "metadata": {
        "id": "5ppRJzlCF_kS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to upload at least a single isochrone file to a new folder named `isochs/`. Once that is done, run the code below to generate the necessary ASteCA objects:"
      ],
      "metadata": {
        "id": "5juGl2nN4mLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a 'cluster' object loading the cluster's data\n",
        "my_cluster = asteca.cluster(\n",
        "    obs_df=df,\n",
        "    ra='RA_ICRS',\n",
        "    dec='DE_ICRS',\n",
        "    magnitude=\"Gmag\",\n",
        "    e_mag=\"e_Gmag\",\n",
        "    color=\"BP-RP\",\n",
        "    e_color='e_BP-RP'\n",
        ")\n",
        "\n",
        "# Load the isochrone(s) file(s). The file(s) must be located in a 'isochs/' folder within this Colab session\n",
        "isochs = asteca.isochrones(\n",
        "    model=\"PARSEC\",\n",
        "    isochs_path=\"isochs/\",\n",
        "    magnitude=\"Gmag\",\n",
        "    color=(\"G_BPmag\", \"G_RPmag\"),\n",
        "    magnitude_effl=6390.7,\n",
        "    color_effl=(5182.58, 7825.08),\n",
        ")\n",
        "\n",
        "# Instantiate a 'synthetic' object using the isochrones\n",
        "synthcl = asteca.synthetic(isochs)\n",
        "\n",
        "# Define the paramters that are fixed, not fitted\n",
        "fix_params = {\"alpha\": 0.09, \"beta\": 0.94, \"Rv\": 3.1, \"DR\": 0.0, \"met\": 0.152}\n",
        "\n",
        "# Calibrate the 'synthetic' object\n",
        "synthcl.calibrate(my_cluster, fix_params)"
      ],
      "metadata": {
        "id": "9pFpMRIIBSfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up the priors and functions required to use the pyABC package. This is set to run for 3 minutes but we can of course change this parameter as desired."
      ],
      "metadata": {
        "id": "GhWWSlbxb8FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters ranges for the priors\n",
        "dm_min, dm_max = 0, 4\n",
        "Av_min, Av_max = 0, 1\n",
        "loga_min, loga_max = 6, 10\n",
        "# Priors dictionary\n",
        "priors = {\n",
        "    \"loga\": [loga_min, loga_max],\n",
        "    \"dm\": [dm_min, dm_max],\n",
        "    \"Av\": [Av_min, Av_max],\n",
        "}\n",
        "# Load priors into pyABC using uniform distributions for all\n",
        "priors = pyabc.Distribution(\n",
        "    **{\n",
        "        p: pyabc.RV(\"uniform\", priors[p][0], priors[p][1] - priors[p][0])\n",
        "        for p in priors\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define likelihood\n",
        "likelihood = asteca.likelihood(my_cluster)\n",
        "max_lkl = likelihood.max_lkl\n",
        "\n",
        "def model(fit_params):\n",
        "    \"\"\"Generate synthetic cluster.\"\"\"\n",
        "    synth_clust = synthcl.generate(fit_params)\n",
        "    # pyABC expects a dictionary from this function\n",
        "    return {\"data\": synth_clust}\n",
        "\n",
        "def distance(synth, obs):\n",
        "    \"\"\"\n",
        "    The likelihood is maximized for better fitted models but this distance requires\n",
        "    minimization. Hence we normalize and invert.\n",
        "    \"\"\"\n",
        "    lkl = likelihood.get(synth[\"data\"])\n",
        "    return 1 - lkl / max_lkl\n",
        "\n",
        "# Define pyABC parameters\n",
        "pop_size = 100\n",
        "max_mins = 3\n",
        "n_procs = 2\n",
        "abc = pyabc.ABCSMC(\n",
        "    model,\n",
        "    priors,\n",
        "    distance,\n",
        "    population_size=pop_size,\n",
        "    sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs),\n",
        ")\n",
        "# This is a temporary file required by pyABC\n",
        "db_path = \"sqlite:///\" + os.path.join(tempfile.gettempdir(), \"pyABC.db\")\n",
        "abc.new(db_path)"
      ],
      "metadata": {
        "id": "OV-p0Av0BoTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and finally we perform a pyABC run:"
      ],
      "metadata": {
        "id": "ZTGsdeGbCrtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run pyABC\n",
        "history = abc.run(\n",
        "    max_walltime=dt.timedelta(hours=0, minutes=max_mins),\n",
        ")\n",
        "# Extract last iteration and weights\n",
        "df, w = history.get_distribution()\n",
        "\n",
        "# Print effective sample size (ESS) and final minimum distance obtained\n",
        "print(f\"\\nESS: {pyabc.weighted_statistics.effective_sample_size(w):.3f}\")\n",
        "final_dist = pyabc.inference_util.eps_from_hist(history)\n",
        "print(f\"Dist: {final_dist:.3f}\")\n",
        "\n",
        "# Define a dictionary of model parameters (those that were not fitted) and their uncertainties\n",
        "fit_model, fit_model_std = {}, {}\n",
        "for k in df.keys():\n",
        "    _median = pyabc.weighted_statistics.weighted_median(df[k].values, w)\n",
        "    _std = pyabc.weighted_statistics.weighted_std(df[k].values, w)\n",
        "    fit_model[k] = _median\n",
        "    fit_model_std[k] = _std\n",
        "    print(f\"{k}: {_median:.4f} +/- {_std:.4f}\")"
      ],
      "metadata": {
        "id": "8isGb7tqCsWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `asteca.plot` method to generate a simple CMD with the resulting best fir synthetic cluster (and its isochrone) found:"
      ],
      "metadata": {
        "id": "xsRdgN8Ib9iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "asteca.plot.cluster(my_cluster, ax1)\n",
        "isoch_arr = asteca.plot.get_isochrone(synthcl, fit_model)\n",
        "asteca.plot.synthetic(synthcl, ax2, fit_model, isoch_arr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LrU6PBRrb9Dn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
